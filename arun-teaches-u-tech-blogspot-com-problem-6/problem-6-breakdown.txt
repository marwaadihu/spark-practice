http://arun-teaches-u-tech.blogspot.com/p/problem-6.html

Provide two solutions for steps 2 to 7
*	Using HIVE QL over Hive Context
*	Using Spark SQL over Spark SQL Context or by using RDDs
==============================================================
1. create a hive meta store database named problem6 and import all tables from mysql retail_db database into hive meta store. 

hive

create database anilagrawal_problem6;

exit;

sqoop import-all-tables \
--connect jdbc:mysql://hostname/retail_db \
--username retail_user \
--password password \
--hive-database anilagrawal_problem6 \
--autoreset-to-one-mapper \
--warehouse-dir /apps/hive/warehouse/anilagrawal_problem6.db/ \
--hive-import \
--hive-database anilagrawal_problem6 \
--create-hive-table

==============================================================
2. On spark shell use data available on meta store as source and perform step 3,4,5 and 6. [this proves your ability to use meta store as a source]

# pyspark --master yarn --conf spark.ui.port=12990
from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext
from pyspark.sql.functions import *

conf = SparkConf().setAppName('problem-6').setMaster('yarn-client')

sc = SparkContext(conf=conf)

sqlContext = SQLContext(sc)

==============================================================
3. Rank products within department by price and order by department ascending and rank descending [this proves you can produce ranked and sorted data on joined data sets]

sqlContext.setConf('hive.metastore.uris','thrift://quickstart.cloudera:9083')

sqlContext.sql('use anilagrawal_problem6')

sqlContext.sql('show tables').show()

result3DF = sqlContext.sql('select * from (select p.*, d.department_id, dense_rank() over (partition by d.department_id order by p.product_price) as rnk_dpt_price from categories c join departments d on c.category_department_id == d.department_id join products p on p.product_category_id == c.category_id) as final order by final.department_id ,final.rnk_dpt_price desc')

==============================================================
4. find top 10 customers with most unique product purchases. if more than one customer has the same number of product purchases then the customer with the lowest customer_id will take precedence [this proves you can produce aggregate statistics on joined datasets]

result4DF = sqlContext.sql('select c.customer_id, count(distinct(oi.order_item_product_id)) as unique_products from customers c join orders o on c.customer_id == o.order_customer_id join order_items oi on o.order_id == oi.order_item_order_id group by c.customer_id order by unique_products desc, c.customer_id limit 10')

==============================================================
5. On dataset from step 3, apply filter such that only products less than 100 are extracted [this proves you can use subqueries and also filter data]

result5DF = result3DF.filter(col('product_id') < 100)

==============================================================
6. On dataset from step 4, extract details of products purchased by top 10 customers which are priced at less than 100 USD per unit [this proves you can use subqueries and also filter data]

result4DF.registerTempTable('top_customers')

result5DF = sqlContext.sql('select p.* from products p join order_items oi on p.product_id == oi.order_item_product_id join orders o on oi.order_item_order_id == o.order_id join top_customers tc on o.order_customer_id == tc.customer_id where p.product_price < 100')

==============================================================
7. Store the result of 5 and 6 in new meta store tables within hive. [this proves your ability to use metastore as a sink]

result5DF.saveAsTable('result5DF')

result6DF.saveAsTable('result6DF')
==============================================================