http://arun-teaches-u-tech.blogspot.com/p/problem-7.html
==============================================================

1. This step comprises of three substeps. Please perform tasks under each subset completely  
* using sqoop pull data from MYSQL orders table into /user/cloudera/problem7/prework as AVRO data file using only one mapper
* Pull the file from \user\cloudera\problem7\prework into a local folder named flume-avro
* create a flume agent configuration such that it has an avro source at localhost and port number 11112,  a jdbc channel and an hdfs file sink at /user/cloudera/problem7/sink
*  Use the following command to run an avro client flume-ng avro-client -H localhost -p 11112 -F <<Provide your avro file path here>>

==============================================================
sqoop import \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--table orders \
--delete-target-dir \
--target-dir /user/anilagrawal/cloudera/problem7/prework \
--as-avrodatafile \
--num-mappers 1

mkdir flume-avro

hadoop fs -copyToLocal /user/anilagrawal/cloudera/problem7/prework/* /home/cloudera/anilagrawal/flume-avro

make a flume agent
------------------------------------------------
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = avro
a1.sources.r1.bind = localhost
a1.sources.r1.port = 11112

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://quickstart.cloudera:8020/user/anilagrawal/cloudera/problem7/sink

# Use a channel which buffers events in memory
a1.channels.c1.type = jdbc

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
------------------------------------------------

flume-ng agent --name a1 --conf-file flume-avro.conf

flume-ng avro-client -H localhost -p 11112 -F part-m-00000.avro
==============================================================

2. The CDH comes prepackaged with a log generating job. start_logs, stop_logs and tail_logs. Using these as an aid and provide a solution to below problem. The generated logs can be found at path /opt/gen_logs/logs/access.log  
* run start_logs
* write a flume configuration such that the logs generated by start_logs are dumped into HDFS at location /user/cloudera/problem7/step2. The channel should be non-durable and hence fastest in nature. The channel should be able to hold a maximum of 1000 messages and should commit after every 200 messages. 
* Run the agent. 
* confirm if logs are getting dumped to hdfs.  
* run stop_logs.
==============================================================

vi flume.conf
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /opt/gen_logs/logs/access.log

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://quickstart.cloudera:8020/user/anilagrawal/cloudera/problem7/step2
a1.sinks.k1.hdfs.fileSuffix = .log

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 200

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

flume-ng agent --name a1 --conf-flie flume.conf